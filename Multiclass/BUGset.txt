
V	输出层的权值调节始终发散；
输出层激活函数为Relu函数；
修正参数失误，直接使用error；
应该使用error/wgtsize；
又因为储存类型的限制，实际更新的时候对于更新项再除以一个wgtsize更为稳妥。

V	隐藏层节点预测值只增不减
某一个属性特征值对应的值总是较大，
所以在第一次迭代更新的时候就会出现权值过大的情况，
此后的更新都是小修小补，不能影响权值的本质改变。
***************************
前提：使用动态学习率
1. 标准差是一组数据平均值分散程度的一种度量。
2. 可以使用相对于平均值的分散程度，作为归一化的标准
综上，可以使用归一化操作度量差距信息作为参数考虑，此时关注的是该特征值的稳定程度。
也可以使用均分分布的方式思考该问题，此时关注的是特征值的分布特征。

特征信息在0-1之间的时候，特征值越小，权值更新越微弱。
1. 对于第一种归一化，会使得原本稳定的数值保持稳定状态，更新操作的意义不大；
2. 对于第二种优化，则中和了原本简单的多元分类状态，该特征集下的多个特征种类之间无分重要性（间隔相等，均匀分布）。更新操作于是更加专注于比较当前特征值集合与其他集合之间的关系，以及该集合对于输出的影响程度，即话语权。
**************
故使用第二种归一化方式。
**如果不使用归一化，在进入6（周日）或者5（周六）之前，输出的状态还是可以在0.6-1之间波动的，但是一旦遇到几个连续的6（周日），该处权值的数值以及该节点的输出数值就会提高到一个难以恢复的level，需要多次极小的实际数值的迭代才可能使其回到正常的水平。

经过以上考虑，最后我们放弃了使用归一化操作，因为对于离散变量而言，数据大小并不能表示程度，因此归一化的作用理论上会有提升，但是绝不能到达一个完美的程度。
因此将离散变量拆散为多个特征值，将多特征值作为补充信息进行分析。

*********************
归一化之前遇到一些问题：
我靠文件里竟然是有？的这什么玩意，那现在就得想办法先把问号添补上，完成数据集的完整性才可以继续修改。
此时使用的数据集为最原始的数据集。


哈哈原来只有22个数据，而且大部分还处于连续变量的分布区域，此时使用两端的均值完全可以应对。


====================================================
		多元分类神经网络算法
====================================================
可能会实现神经网络算法，如果时间比较紧的话。

1. 数据集处理，数据中有许多不必要的语法结构，这些结构可以表达所属关系，但是与当前的情感状态相距较远。
真正表达情感信息的词汇应该是 谓语、定语、补语；
所以需要将其分离出来，并且消除掉其他词汇的信息影响。
==
多元分类中选择的词一定要有针对性与稀疏性，
很简单因为对于日常生活中的一些语句，
我们需要根据生活经验来判断，
没有生活经验就不能理解――机器是没有生活经验的，
对此，计算机只能筛选掉所有非情感类的词汇。
使用介词、副词、连词作为标记点分割句子可以简化句子成分。
削弱句子结构，再使用comment类对剩余信息进一步分析，
以期望得到最为精简，最为明显的信息。
反例（人都无法理解的评语，妹妹，我真是服了你了）：
i usually only give 1 star to places so i guess that means i 'd go back right ? <sssss> yea - i 'd go back
分析的时候受限于模型的结构，只能根据词汇的组合判断，
很难辨别得了排列的信息。

现在已经把句子实现了解构，
在分离后的部分中的词语主要是用于阐释
一件事、一种状态或者一种情感，
由是，需要对解构后的信息进行修正，
强化表达情感的部分，增加否定词汇以及形容词的比例。
鉴于词汇的复杂性以及容量，
修剪同种词汇的数量看起来是非常有必要的。
1. 词汇对于电脑来说是完全不可理解的；
	认知表语这件事听起来其实是非常吸引人的，
	但是事实上，在句子中非表语位置
	也可以安置表达情绪的词，
	同样的，
	在表语位置高频使用形容词但是也不尽然，
	完全可以说是there is句式；
	深究起来如果是感叹句那就可以随便瞎说不讲语法了。
	不过笔者还是实现了一点表语收集的函数。
	在教授我的计算机学会一丁丁语法。
2. 词汇的意义只在于其出现的频率；
	
3. 可能会出现词库中尚未收纳的信息
	过滤名词很有必要，或者我们根本就不要使用黑名单，
	尝试去使用白名单，只选择我们认识的单词。
	对不认识的词的态度可以是装瞎，不理会，
	根据已知的信息获得当前语句的情感。

	如果一个句子中的内容完全不理解，
	情况一：生活经验分析，也可以掌握，
		但是难度很大，干扰项多；
	情况二：不认识词，那你就算是人也没有办法；
		更何况对于笔者的PC机来说，
		笔者是绝对不会牺牲稀疏度指标的。
		电脑跑不起。
4.对于稀疏度的讨论
	其实稀疏度信息，是非常重要而且非常灵活的。
	但是由于时间关系，
	笔者仅实现了稀疏度中的表语结构，
	这并不是思考方向上的问题，
	这是笔者算力的限制。
	
将收集的词汇转化为矩阵：
首先需要使用小数据集实现句子的收录。
然后是对句子进行拆分，获得大量的单词，
从中筛选出我们需要的单词。
根据单词组成参考词库。
由参考词库+出现频率组成TF-IDF矩阵。

从评论中获取信息更新信息；

整理当前的神经网络模型的时候，
训练集与测试集使用同样的数据集，
此时注意到low的上限是0.38X，
笔者感觉这个数字非常熟悉――没错，是黄金分割率，
同样的，HIGH 的下限正是0.618，
这种情况出现的原因是两个参量的比例相同。。。
在哪里呢？
对于每一层的神经层进行学习率修改的时候，
因为不同的神经元有不同的学习率，以此形成层次结构，
两层的结构就会出现两个参量比例相同的情况，
原来是在这里表示出来了。

检测发现对于大于当前数值的变化，收敛速度过慢，
这不是我的模型的风格――改！
――将学习率乘以当前层的神经节点的个数，
恰好可以拉开差距。

对于20个的训练集，需要迭代30次以上才可以在原数据集上获得较高的准确率――即记住训练集中的数据特征。
这对于50000个数据来说压力过大，
提出使用集合训练的方法，专项训练数据集。
基于
“多次迭代的本质就是强化神经元，过多次迭代就是过度学习”
	X	由小数据集测试发现效果并不明显，决定放弃这种方法，转而修改计算模型，实现轻量级运行。

精简使用的特征值集合，
1. 去除掉不使用的dictionary；	V
2. 使用卡方检测分析数学相关性与语言语法之间的关系；

迭代次数需要很高才可以拟合模型：
1. 数据的识别率不高；
卡方检验的目的是测量词与当前的文本分类之间的相关度，更具有参考意义，而且查找的词数更少，含金量更高。
同时由于缩小了数据集，可以最大程度上地加快迭代速度。

卡方检验：
对每一个词遍历文本集，计算出现该词的数量；
通过计算与理论值的偏差获得卡方衡量值，
以此决定词汇的重要程度。
词汇数量：90000；
文本数量：90000；
计算偏差：每个词需要计算3遍，分别针对HIG/MID/LOW；
方差衡量：浮点数平方；
此时获得一个90000的序列，表示每个词的重要程度；
筛选前多少列――可能需要排序；
结论：计算复杂度过高。

语法拓展：
在表语中选择最常出现的文本词汇，
维护一个整数序列存放表语词汇的次数，
并根据最高出现次数设定阈值，
通过一次遍历获得出现次数最多的词汇。
大幅度节约运行内存消耗，从500+M到133M

*词频代替统计学中的方差计算：
常出现的词汇表明是人类经常使用的词汇，
从统计学上来讲，参考意义更大。
*根据最高的出现次数设定阈值
这个其实比较主观，因为我们并不知道对于一个文本集而言，
多少词汇才可以真正表示该文本集的分类，
多少词汇才真正是文本集的骨骼，是文本集的灵魂，
所以这里笔者的使用的时候也是比较主观地去使用，
因为可能这种问题就是因文本而异吧。


2. 算法的收敛速度慢；

指标一：同样训练集的准确率，确定是否拟合；
    始终存在一些数据是不可训练的，
有一些mid的数据训练1000次也会预测为HIG，
这说明采样的关键词序列被训练集中的大部分样本影响；
同样的词出现之后，其他的文本是HIG，
而且这种文本所占的比例还非常高，
这就直接导致了这种关键词的组合反馈的分类是HIG；
此时的数据已经不能作为检测模型是否符合期望的输入了。
综上，训练集需要大量的数据
来确定关键词集合的正常的输出label；

指标二：不同数据集的测试度，确定模型状态；
预测的值通常都在0.5之下，迭代几十次都没有变化：7/21
选取的特征词汇数量需要可以恰好反映当前的文本集合词汇，
过少会导致错误的判决，过多又添加噪声，减慢收敛速度；
现象：
*迭代可以拉开分类之间的预测指标，即输出节点的小数，
*过多的迭代导致分类之间的指标再次靠近
 ――输出的值有时候为负值，
     意味着误差较大而产生的大幅度修正；
*迭代195次果然出现了差值的反复，而且差值的分布更符合期望，
 如果对于返回值，
 若值为0.5，则更新迭代难以达到0.5；
 若值为1，则更新迭代难以达到1；
 若值为0，则更新迭代甚至可以是负数
	――因为初始化的时候输出>>0，
	由于误差的存在过大，出现了过度校正的现象。
*但是即使是这样，整体还是偏低，为什么呢
指标一：整体水平偏低，
V	将节点的偏置项设置为0.5，
	则整体预测输出都上升0.5，
	此时的值都是在0.5上下波动

指标二：预测输出的极值大小：
常常会出现低于0.1的值，但是很少出现高的值，
发现在数据集中，low几乎占了一半的水平，
为此提高HIG的标准值，保证回传的合理性。
 
指标三：HIG的预测准确度
此外还有对于MID的预测可能会低于LOW的情况
 （目前使用1/4的训练集使用内存100+M）
反而是对于HIG的预测格外准确：
迭代55次，
可以14个可以识别出13个这样子
可以识别出39个中的31个，准确率大概是75%，并且还可以迭代
可以识别55个中的51个low，准确率大概是92%，
	但是同时也对大量的MID分类为LOW
但是HIG的预测受到了动词成分的干扰，
文中多次出现Love就会掩饰掉其表达的情感。
所以手动添加Love设定，好在这种用法也不是很多。

HIG中有一个非常重要的指标就是LOVE，
但是神经网络对于love是不能认知的，
因为其不属于表语范畴，
因此需要使用申请网络可以数字化的信息对其进行解释，
把所有最美好的词都作为对于love的补充，
尝试去代替其原有的位置，
我知道那是不够的，但是我也不知道love是什么，
而动词系列中，我也只添加了love，
because what we need, is love.

由于指标的受限，目前在验证集上的表现为：
迭代5次，准确率66%；
仅供参考
// 12.30提交：v1_fout, v2_low, v3_mid, v4_hig

指标四：
分算各个分类的准确率，因为无法测量test集中数据的分布；
三者计算的准确度都不高，
[main]:hig/HIG=33/48
[main]:mid/MID=14/99
[main]:low/LOW=39/53
为了提升效果，准备使用adaboost；
使用adaboost之前，需要确定模型没有饱和，
也就是模型承受来自于adaboost的额外的计算量
而不会影响已经训练得到的算法模式。

实现这个部分在已有的bound基础上，
看在该基础上可以提升多少；为了试验结果更加明显，
这里将mid区域划分得更宽，以检测两端的划分准确度
――事实上，这才是分类中最为核心的部分。
分离效果没有更好；

在第三次训练遍历的时候使用adaboost针对性训练，
对每一个错误的数据迭代5次使之收敛――5次就算是麻花也收敛了
可能需要增加迭代，以查询当前模型是否已经饱和或者趋于饱和。

指标五：
设置划分的bound，在划分之前需要知道大致的分类分布，
因而需要一些反馈数据，去估算在测试集中分类的数量级；
中间部分太少，这说明中间区间过于狭窄。、

经过测试，猜测三种label的分类大致为：
LOW 3321
MID 1690
HIG 3670


指标六：
调节阈值。
预测标签与滞后效应，由于训练集中的标签不能过大（<1.4）;
而迭代使得预测值不断地趋向于该标签但是永远不能达到，
因此会出现非常明显的滞后效应，
现象：
MID预测的准确度较高，LOW的次之，因为区域有0.30+，
HIG的最小；

根据不等式法则，需要使得三种情况的预测准确度相近，
才可以在平均的时候获得较大的平均值，
又由于三者的数量相近，比例适中。
因此调节阈值

0.302/0.568：
[main]:hig/HIG=22/53
[main]:mid/MID=49/99
[main]:low/LOW=33/48

调节阈值为0.302/0.478：
[main]:hig/HIG=34/53
[main]:mid/MID=41/99
[main]:low/LOW=29/48
MID所占的比例过小，需要拓展
0.302/0.518
[main]:hig/HIG=25/53
[main]:mid/MID=48/99
[main]:low/LOW=37/48
可以了，比较适中，但是随着迭代次数的上升，
平均值也会不断地上升，因此，如果迭代次数有明显激增的话，
需要提升阈值。
当前使用内存394M，迭代15次计算时间1:50，shoot110/200；



++++++++++++++++++++
1. 其实比较常规的梳理方法恰恰是使用情态动词，
因为情态动词后表意愿，其后会有许多固定的常见用法，
但是这些常见用法对于笔者来说也是很难整理出来，
所以笔者就只是尝试去削弱句子的复杂度，
根据形容词进行分类。

2. 数据竟然有Unicode字符！我是不服气的，我要给差评。
为了解决问题，只能遍历一次，整理数据之后，生成新的文件以便使用，因为所有相关的函数都是应对常规字符的。

3. 预测之前
- 调整采词的数量，满足涵盖关系
- 调整分类的阈值，识别分类效果
- 先跑一遍valid.csv集合数量为200；
  调整参数之后，再运行test.csv；

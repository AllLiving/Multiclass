
实现这个部分在已有的bound基础上，
看在该基础上可以提升多少；为了试验结果更加明显，
这里将mid区域划分得更宽，以检测两端的划分准确度
――事实上，这才是分类中最为核心的部分。
这里使用0.282/0.618

理论值足够分离以保证不会混叠，但是初步的计算结果显示：
收敛的程度还是远远不够。因而需要强化收敛：
迭代15次，mstFreq/15
对cnt==3的错误数据收集，错误数据每个迭代两次：
[main]:hig/HIG=25/48
[main]:mid/MID=54/99
[main]:low/LOW=24/53
初步观测到收敛效果显示初步处于50%的层次上，
观察到数据对于MID和HIG分辨的不是非常明显，
可能是传入参数不够多,增加输入节点的数量到178；
[main]:hig/HIG=29/48
[main]:mid/MID=55/99
[main]:low/LOW=28/53
水平平均上升嘻嘻嘻，但是计算的复杂度也直接上升，内存亦然；
输入节点为214时候，即针对227个词进行识别的时候，出现下降，
这种下降情况的出现的解释是：
新引入的词已经对神经的判断造成了负面的影响，
即引入了高频出现的噪音词。mstFreq/25
现象：
1. MID被预测为HIG，因为有非表情感的词被纳入考虑，
[main]:hig/HIG=32/48
[main]:mid/MID=48/99
[main]:low/LOW=19/53
可能是造成了干扰，也有可能是随着复杂度的增加，迭代次数不足以收敛。
实验检测后一种可能：
升高迭代次数至95次：
============mstFreq/15 111nodes
[main]:hig/HIG=23/48
[main]:mid/MID=48/99
[main]:low/LOW=33/53
104，到达瓶颈
============mstFreq/25 212nodes
[main]:hig/HIG=30/48
[main]:mid/MID=45/99
[main]:low/LOW=28/53
103，收敛缓慢；
最终确定选择mstFreq/25，即输入节点为212；

接下来调整针对性训练:
1. 将Adaboost内部的重复训练增加次数。
2. 消减迭代次数到15次：
[main]:hig/HIG=32/48
[main]:mid/MID=48/99
[main]:low/LOW=19/53
没有提升，增加针对性迭代数量至5。
[main]:hig/HIG=33/48
[main]:mid/MID=46/99
[main]:low/LOW=17/53
显示有负面影响，这说明模型承受额外的计算量的时候产生干扰

添加新的改进，将love的定义修改为5个常见的形容词，
迭代15次，得到：
[main]:hig/HIG=13/48
[main]:mid/MID=60/99
[main]:low/LOW=32/53
输入节点212； 
调整隐藏层节点的学习率梯度为n^2，low与MID的准确度都上升
[main]:hig/HIG=15/53
[main]:mid/MID=55/99
[main]:low/LOW=31/48

这种调整使得输出节点的输入端口的数据权值可以拉开差距，
如此，在增加隐藏层节点的时候，
隐藏层节点的数据层次化更加明显，
由此，划分的操作在保证细致状态的同时可以允许更大的输出范围

此时将隐藏层节点添加为55个，
得到15次迭代：
针对失误的判断重复迭代3次的情况下，
有准确率如下：
[main]:hig/HIG=16/53
[main]:mid/MID=55/99
[main]:low/LOW=31/48

iterator = 25:
[main]:hig/HIG=16/53
[main]:mid/MID=52/99
[main]:low/LOW=34/48

iterator = 10:
[main]:hig/HIG=21/53
[main]:mid/MID=55/99
[main]:low/LOW=31/48

最终确定隐藏层节点数量大于55，初步确定为65/78；

Adaboost拓展作用范围：
在Adaboost仅仅作用在第3个位置的情况下，
实验结果如下：
[main]:hig/HIG=22/53
[main]:mid/MID=49/99
[main]:low/LOW=33/48
针对前10个位置使用adaboost效果如下：
[main]:hig/HIG=18/53
[main]:mid/MID=62/99
[main]:low/LOW=31/48
效果显著，得出结论：
需要增加Adaboost的作用范围，
同时增加迭代次数；




